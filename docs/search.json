[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi, I’m Donald\nSince 2017, I’ve held the position of Biostatistician at IDEXX Laboratories within the Medical Data Insights team. My responsibilities predominantly revolve around three main domains:\n\nClinical Studies\nReal World Evidence (RWE) Studies\nValidation Studies for machine learning algorithms\n\nAs part of my work, I collaborate with over 40 universities and research institutions across 20 different countries:\n\nPreviously, I worked at Harvard Innovation Labs as a biostatistician consultant and Harvard Medical School/Beth Isreal Deaconess Medical Center as a biostatistician focused on large cardiovascular clinical trials.\nI received a Bachelor’s of Science in Human Biology with minors in Biochemistry and Physics from the University of Southern Maine. The focus of my undergraduate work was on the effects of heavy metals (arsenic) on the developing cytoskeleton of rat pheochromocytoma cells as a model for brain development.\nIn addition, I obtained both an undergraduate and graduate research fellowship from NASA focused on the effects of space radiation on developing glial cells and the mitigation of stunted neurite outgrowth through the use of antioxidants to mitigate damage from reactive oxygen species at the Biomedical Research and Operations branch of Johnson Space Center.\nMy graduate research focused on the implementation of a a clinical decision support system for traumatic brain injury (the Canadian Head CT scan rule) in a single large hospital and the evaluation of the performance of that implementation into an EHR."
  },
  {
    "objectID": "posts/002_blog_variance-sum-law/index.html",
    "href": "posts/002_blog_variance-sum-law/index.html",
    "title": "Variance Sum Law through R",
    "section": "",
    "text": "The variance sum law states that the expectation value of the sum of two independently random variables (\\(x\\) and \\(y\\)) equal the sum of the expectation values of the two variables:\n\\[\\begin{align}\n    var(x+y) = var(x)+var(y)\n\\end{align}\\]\nTo try and code this in R you might start using the stats::rnorm() function like so:\n\n\nx <- stats::rnorm(100)\n\ny <- stats::rnorm(100)\n\nvar(x) + var(y)\n## [1] 2.161509\n\nvar(x + y)\n## [1] 2.109248\n\nWhat is going on here? It does not look as though the variance sum law works in the case shown above. Is it possible that these two variables are not completely independent from one another?\nLet’s take a look:\n\n\ncor.test(x,y)\n## \n##  Pearson's product-moment correlation\n## \n## data:  x and y\n## t = -0.24075, df = 98, p-value = 0.8102\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.2196817  0.1729313\n## sample estimates:\n##         cor \n## -0.02431262\n\nThey are correlated! Because we did not ensure that the two variables are completely random with respect to one another, they are slightly correlated with each other.\nLets play around with this a little. How much correlation we would expect between two random variables? Why don’t we simulate a distribution of random variables and see how correlated these to random variables are:\n\nset.seed(123)\n\nvector_list <- 1:10000\n\ncorr_data <- data.frame()\nfor (variable in vector_list) {\n  \nx <- stats::rnorm(100)\n\ny <- stats::rnorm(100)\n\ncor_est <- cor.test(x,y)[4]\n\ncorr_data <- rbind(corr_data,cor_est)\n  \n}\n\nggplot(corr_data, aes(x = estimate)) +\n       geom_histogram(bins = 100, color = \"black\", fill = \"lightgrey\") +\n       scale_x_continuous(\"Correlation Estimate\", expand = c(0,0)) +\n       scale_y_continuous(\"Frequency\", expand = c(0,0))\n\n\n\n\n\nround(max(corr_data$estimate),2)\n## [1] 0.36\n\nround(min(corr_data$estimate),2)\n## [1] -0.41\n\nWhile the 10,000 simulated values appear to be centered around zero, there is quite a large spread in the results with correlation coefficient as low as -0.41 and as high as 0.36!\nNow that we know the reason the variance sum law doesn’t seem to work in this case, how can we generate two random variables two with correlation between them?\nWe can start by taking a look at the differences in the correlation of these values using a least squares regression model and exploring the residuals.\n\n\nset.seed(666) #set seed rock on \\m/\ny <- rnorm(10)\n\nx <- rnorm(10)\n\nlm <- lm(y ~ x)\n\n#show lm coefficients\nlm$coefficients\n## (Intercept)           x \n## -0.04818839 -0.48394162\n\n# pull out the slope of the regression model\nslope <- lm$coefficients[2]\n\n# pull out the y-intercept of the regression model\nintercept <- lm$coefficients[1]\n\n#get the predicted valus from the regression model\nyhat <- lm$fitted.values\n\n# difference between predicted values and actual (residuals)\ndiff_yx <- y-yhat\n\n# plot data - qplot allows the use of vectors over data frames\nqplot(x=x, y=y)+\n      # plot regression slope\n      geom_abline(slope = slope, intercept = intercept)  +\n      # add the residuals\n      geom_segment(aes(x=x, xend=x, y=y, yend=yhat), color = \"red\", linetype = \"dashed\") +\n      # plot points \"pch\" just allows me to fill in the points\n      geom_point(fill=\"white\",colour=\"black\",pch=21)\n\n\n\n\n\n # plot table of data points, predicted values (yhat) and residuals (diff_yx)\nknitr::kable(data.frame(x,y,yhat,diff_yx))\n\n\n\n\nx\ny\nyhat\ndiff_yx\n\n\n\n\n2.1500426\n0.7533110\n-1.0886835\n1.8419945\n\n\n-1.7702308\n2.0143547\n0.8085000\n1.2058547\n\n\n0.8646536\n-0.3551345\n-0.4666303\n0.1114958\n\n\n-1.7201559\n2.0281678\n0.7842666\n1.2439012\n\n\n0.1341257\n-2.2168745\n-0.1130974\n-2.1037771\n\n\n-0.0758266\n0.7583962\n-0.0114928\n0.7698889\n\n\n0.8583005\n-1.3061853\n-0.4635557\n-0.8426295\n\n\n0.3449003\n-0.8025196\n-0.2151000\n-0.5874195\n\n\n-0.5824527\n-1.7922408\n0.2336847\n-2.0259255\n\n\n0.7861704\n-0.0420325\n-0.4286490\n0.3866165\n\n\n\n\n\nIn the least square regression of \\(x\\) against \\(y\\), the residuals represent the removal of the \\(y\\) component from \\(x\\), giving a column of values that are orthogonal (i.e. at right angles) to the values of \\(y\\). We can add back in a multiple of \\(y\\) that will give us a vector of values with our desired correlation. Since we are looking for a correlation of zero, the \\(y\\) component that we are adding back in is the multiple the standard deviation and the residual around our \\(y\\) value.\n\n\n# the residual of x against y (not to be confused with y against X from above)\ndiff_xy <- residuals(lm(x~y))\n\n\n# our new x value which is the is the multiple the standard deviation and the residual around our y value\nx2 <- diff_xy*sd(y)\n\n\n# correlation between our y value and our new x value\ncor(y,x2)\n## [1] -4.778814e-17\n\n\n#round to 5 digits\nround(cor(y,x2),5)\n## [1] 0\n\nWoohoo! Now that we have our new random variable (\\(x_2\\)) with no correlation against our \\(y\\) values. Lets try to run the variance sum law again.\n\n\nvar(x2) + var(y)\n## [1] 4.930332\n\nvar(x2 + y)\n## [1] 4.930332\n\nAwesome! Now we have been able to show that the expectation value of the sum of two independently random variables (\\(x\\) and \\(y\\)) equal the sum of the expectation values of the two variables!\nAnd just for the heck of it, lets turn that code into a function for future use!\n\n\n\n# functionalizing the code above\nno_corr_variable <- function(y, x) {\n  diff_xy <- residuals(lm(x ~ y))\n  diff_xy * sd(y)\n}\n\n\nSupplementary: Expansions to other Correlations\nThere has actually been a lot of work done around expanding this problem for different distributions and correlations. In fact, the simple program we wrote for a correlation of zero was simplified from a more generalized equation. A discussion around the generalization and expansion of this problem can be found on CrossValidated.\nThe generalized form:\n\\(X_{Y;\\rho} = \\rho SD (Y^{\\perp})Y + \\sqrt{1- \\rho^2}SD(Y)Y^{\\perp}\\),\nwhere vector \\(X\\) and vector \\(Y\\) have the same length, \\(Y^{\\perp}\\) is the residuals of the least squares regression of \\(X\\) against \\(Y\\), \\(\\rho\\) is the desired correlation, and \\(SD\\) stands for any calculation proportional to a standard deviation.\nSince we were looking for a situation in which \\(\\rho = 0\\) the above equation can be simplified as follows:\n\\(X_{Y;\\rho=0} = (0) \\cdot SD (Y^{\\perp})Y + \\sqrt{1- (0)^2}SD(Y)Y^{\\perp}\\)\n\\(X_{Y;\\rho=0} = \\sqrt{1}SD(Y)Y^{\\perp}\\)\n\\(X_{Y;\\rho = 0} = SD(Y)Y^{\\perp}\\)"
  },
  {
    "objectID": "posts/003_blog_tidymodel-mixed-effects/index.html",
    "href": "posts/003_blog_tidymodel-mixed-effects/index.html",
    "title": "A basic tutorial on plotting mixed effects using tidymodels",
    "section": "",
    "text": "In R there are a ton of packages available to regression models including mixed effects model but one of the biggest issues is the vast difference in syntax needed for each of the modeling packages. Tidymodels was developed to solve this problem with the goal of having similar syntax style to the other tidyverse packages. Tidymodels itself is a “meta-package” consisting of a bunch {https://tidymodels.tidymodels.org/} of packages for modeling and statistical analysis with a focus on using the design philosophy of the tidyverse packages.\nOne of the current packages in development (as of this blog post) is the multilevelmod package for hierarchical modeling.\nIn this tutorial I am going to go through how to create a mixed effects model in R using the tidymodels and multilevelmod packages and how to plot the random intercepts using ggplot2. This blog post is just focused on using tidymodels and is not an indept overview of what mixed effects models are or how to use them.\nFirst lets load our packages of interest. For the multilevelmod package (as of the time of this blog post) you will need to install it through github using devtools::install_github(). I ended up running into a little bit of a problem with an out of date Rcpp package. Deleting the folder manually and then re-installing ended up doig the trick. We will also be loading in one of R’s most used mixed effects modeling packages,lme4, to get some data.\n\n#install.packages(\"pacman\")\n#load required packages\npacman::p_load(\"tidyverse\",\"lme4\")\n\n# install multilevelmod\n#devtools::install_github(\"tidymodels/multilevelmod\")\n\nlibrary(\"multilevelmod\")\n\nLoading required package: parsnip\n\n\nNow lets load in the sleep study dataset from the lme4 package. I am also going to create a fake categorical variable to use as a fixed effect in the model.\n\n# load sleep study and create a fake category\nset.seed(666) #\\m/ rock on\nsleepstudy <- lme4::sleepstudy %>% group_by(Subject) %>% \n              # creating a new random category with 3 levels to explore group effects\n              mutate(cat = sample( LETTERS[1:3], 1, replace=TRUE, prob=c(0.25, 0.50, 0.25))) %>%\n              ungroup()\n\nThe Tidymodels syntax requires that we set the “engine” for the type of model that we want to use. It is kind of like picking the type of car to use in MarioKart. You want to use the right engine for the right type of data. Since we are interested in looking at repeated measures data using a mixed effects model we will be using the “lmer” engine. We can set up the engine with the following code:\n\n# set engine for mixed effects models\nmixed_model_spec <- linear_reg() %>% set_engine(\"lmer\")\n\nNext we can build the model using Reaction as our dependent variable, Days and cat as our fixed effects and Subject as our random effect. The random effect syntax following that of the lme4 package using (1|Subject) to define Subject as the random intercept.\n\n# create model\nmixed_model_fit_tidy <- mixed_model_spec %>% fit(Reaction ~ Days + cat + (1 | Subject), data = sleepstudy)\n\nmixed_model_fit_tidy\n\nparsnip model object\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ Days + cat + (1 | Subject)\n   Data: data\nREML criterion at convergence: 1769.298\nRandom effects:\n Groups   Name        Std.Dev.\n Subject  (Intercept) 39.58   \n Residual             30.99   \nNumber of obs: 180, groups:  Subject, 18\nFixed Effects:\n(Intercept)         Days         catB         catC  \n    248.683       10.467        3.407       11.516  \n\n\nNow that we have created our model. Lets take a look at the predicted probabilities. To do this, we will create a data frame with all the different combinations of our fixed and random effects.\n\nexpanded_df_tidy <- with(sleepstudy,\n                    data.frame(\n                      expand.grid(Subject=levels(Subject),\n                                  #cat = unique(cat),\n                                  Days=seq(min(Days),max(Days),length=51))))\n\nexpanded_df_tidy <- sleepstudy %>% tidyr::expand(Subject,Days,cat)\n\nexpanded_df_tidy\n\n# A tibble: 540 × 3\n   Subject  Days cat  \n   <fct>   <dbl> <chr>\n 1 308         0 A    \n 2 308         0 B    \n 3 308         0 C    \n 4 308         1 A    \n 5 308         1 B    \n 6 308         1 C    \n 7 308         2 A    \n 8 308         2 B    \n 9 308         2 C    \n10 308         3 A    \n# … with 530 more rows\n\n\nWe can use this data frame and the predict() to get the predictions from our model.\n\npredicted_df_tidy <- mutate(expanded_df_tidy,\n                            pred = predict(mixed_model_fit_tidy,\n                                           new_data=expanded_df_tidy, \n                                           type = \"raw\", opts=list(re.form=NA)))\n\n\npredicted_df_tidy\n\n# A tibble: 540 × 4\n   Subject  Days cat    pred\n   <fct>   <dbl> <chr> <dbl>\n 1 308         0 A      249.\n 2 308         0 B      252.\n 3 308         0 C      260.\n 4 308         1 A      259.\n 5 308         1 B      263.\n 6 308         1 C      271.\n 7 308         2 A      270.\n 8 308         2 B      273.\n 9 308         2 C      281.\n10 308         3 A      280.\n# … with 530 more rows\n\n\nWhen looking at the prediction output, notice that we are getting the same predictions for each subject. The predict function is currently giving us predictions for the fixed effects. If were were to run this same code using predict() with lme4 we would get the predictions for the random effects for each `Subject.\nWhat is going on here? The issue is that multilevelmod package internally sets the default for prediction to re.form = NA;. In lme4 the default for predictions is re.form = NULL (i.e. include all random effects in the prediction).\n\nknitr::include_graphics(\"tidymodel_git_comment.PNG\")\n\n\n\n\nWe can include re.form = NULL in the predict() function by using the opts argument.\n\n#update predictions\npredicted_df_tidy <- mutate(expanded_df_tidy,\n                            # get random predictions\n                            pred_rand = predict(mixed_model_fit_tidy,\n                                                new_data=expanded_df_tidy, \n                                                type = \"raw\", opts=list(re.form=NULL)),\n                            # get fixed effect predictions\n                            pred_fixed = predict(mixed_model_fit_tidy,\n                                                new_data=expanded_df_tidy, \n                                                type = \"raw\", opts=list(re.form=NA)))\n\npredicted_df_tidy\n\n# A tibble: 540 × 5\n   Subject  Days cat   pred_rand pred_fixed\n   <fct>   <dbl> <chr>     <dbl>      <dbl>\n 1 308         0 A          292.       249.\n 2 308         0 B          296.       252.\n 3 308         0 C          304.       260.\n 4 308         1 A          303.       259.\n 5 308         1 B          306.       263.\n 6 308         1 C          314.       271.\n 7 308         2 A          313.       270.\n 8 308         2 B          317.       273.\n 9 308         2 C          325.       281.\n10 308         3 A          324.       280.\n# … with 530 more rows\n\n\nNow that we have both the predictions for the fixed and random effects we can plot them using ggplot2!\n\nggplot(predicted_df_tidy) +\n       facet_wrap(.~cat) + \n       geom_line(aes(x=Days,y=pred_fixed), size = 1) + \n       geom_line(aes(x=Days,y=pred_rand,colour=Subject)) +\n       scale_y_continuous(\"Predicted\") + \n       theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Hello, World!\nWelcome to my blog!\nWhile this blog will be mainly focused on statistics, I am (at least hoping) to have this blog broken up into a few sections!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Introduction to the Cessie-Houwelingen Test Statistic\n\n\n\n\n\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2023\n\n\nDonald Szlosek\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Simulated Annealing in R\n\n\n\n\n\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\nDonald Szlosek\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2023\n\n\nDonald Szlosek\n\n\n\n\n\n\n  \n\n\n\n\nA basic tutorial on plotting mixed effects using tidymodels\n\n\n\n\n\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2020\n\n\nDonald Szlosek\n\n\n\n\n\n\n  \n\n\n\n\nVariance Sum Law through R\n\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2020\n\n\nDonald Szlosek\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Rarely do we see linear relationships in nature, yet a lot of research is modeled using linear models. My research focuses on the use of non-linear models to better understand the complex relationship between between biomarkers and disease.\nIn addition, I also focus on the development and application of improved methods for handling of imperfect hold standards in the training and validation of machine learning algorithms, with the goal of building more robust medical AI systems."
  },
  {
    "objectID": "posts/004_blog_intro_simulated_annealing/index.html",
    "href": "posts/004_blog_intro_simulated_annealing/index.html",
    "title": "Introduction to Simulated Annealing in R",
    "section": "",
    "text": "Simulated annealing is a stochastic optimization algorithm that draws inspiration from the annealing process in metallurgy, where metals are heated and slowly cooled to achieve a low-energy crystalline state. The goal is to find an optimal solution to a problem by exploring a solution space and “cooling down” over time. Here’s a mathematical representation of the key components and the intuition behind simulated annealing:\n\nObjective Function: In optimization problems, we have an objective function \\(f(x)\\) that assigns a value to each candidate solution \\(x\\). The objective is to either maximize or minimize this function.\nSolution Space: The set of all possible solutions to the problem is called the solution space, denoted as \\(X\\). Each point \\(x\\) in this space corresponds to a specific solution.\nEnergy Landscape: The objective function defines an “energy landscape” over the solution space. Higher values of the objective function correspond to higher energy states, and lower values correspond to lower energy states. In some cases, the landscape may have multiple local optima, making it challenging to find the global optimum.\nTemperature: Simulated annealing introduces the concept of temperature (\\(T\\)), which is initially set to a high value. The temperature controls the exploration versus exploitation trade-off. Higher temperatures allow for more exploration (acceptance of worse solutions), while lower temperatures focus on exploitation (acceptance of better solutions).\nNeighbor Generation: To explore the solution space, we generate neighboring solutions from the current solution. This is done using a “neighbor generation” function, \\(x' = \\text{generate\\_neighbor}(x)\\), which perturbs the current solution.\nAcceptance Probability: Simulated annealing accepts or rejects neighboring solutions based on a probability distribution. The probability of accepting a worse solution (\\(x'\\)) is determined by:\n\n\\[\nP(\\text{accept}) = \\begin{cases}\n1, & \\text{if } f(x') &lt; f(x) \\quad \\text{(worse solution)} \\\\\ne^{-\\frac{f(x') - f(x)}{T}}, & \\text{if } f(x') \\geq f(x) \\quad \\text{(better solution)}\n\\end{cases}\n\\]\nHere, \\(e\\) is the base of the natural logarithm, and \\(T\\) is the current temperature. When \\(T\\) is high, the algorithm is more likely to accept worse solutions, enabling exploration. As \\(T\\) decreases, it becomes less likely to accept worse solutions, focusing on exploitation.\n\nCooling Schedule: The temperature is reduced gradually over time according to a “cooling schedule.” A common choice is to decrease the temperature exponentially:\n\n\\[\nT_{\\text{new}} = \\alpha \\cdot T_{\\text{old}}\n\\]\nHere, \\(\\alpha\\) is the cooling rate (typically between 0 and 1).\n\nTermination: The algorithm iterates through these steps for a specified number of iterations or until a termination condition is met (e.g., a low temperature).\nExploration History: Throughout the process, the algorithm maintains a record of the solutions explored. This exploration history can be visualized to understand how the algorithm traversed the solution space.\n\nBy combining these components, simulated annealing gradually explores the solution space, initially accepting worse solutions with a higher probability and later focusing on exploiting better solutions as the temperature decreases. This stochastic exploration allows it to escape local optima and converge to a near-optimal solution.\nYou can use this simulated annealing to morphs between shapes, by defining The sequence of shapes symbolizes the solution space. Unlike traditional acceptance probabilities, this code uniformly generates and visualizes intermediate shapes for a comprehensive exploration. Although there’s no cooling schedule, the gradual and continuous transitions mimic the annealing process. The more steps you allow, the “smoother” the transition (althorugh the time to run the code gets much longer).\nHere is some code that transitions from a circle to a line:\n\n# load dependencies\n#install.packages(\"pacman\")\npacman::p_load(\"ggplot2\",\"gridExtra\",\"ggplot2\",\"gganimate\",\"transformr\",\"gifski\")\n\n\n# Function to generate circle points\ngenerate_circle &lt;- function(n) {\n  theta &lt;- seq(0, 2 * pi, length.out = n + 1)[1:n]\n  x &lt;- cos(theta)\n  y &lt;- sin(theta)\n  data.frame(x, y)\n}\n\n# Function to generate horizontal line points\ngenerate_line &lt;- function(n) {\n  x &lt;- seq(-1, 1, length.out = n)\n  y &lt;- rep(0, n)\n  data.frame(x, y)\n}\n\n# Function to morph between two shapes\nmorph_shapes &lt;- function(shape1, shape2, steps) {\n  morphed_shapes &lt;- vector(\"list\", steps)\n  for (i in 1:steps) {\n    alpha &lt;- (i - 1) / (steps - 1)\n    morphed_shapes[[i]] &lt;- shape1 * (1 - alpha) + shape2 * alpha\n  }\n  morphed_shapes\n}\n\n# Number of points and steps\nn &lt;- 50\nsteps &lt;- 20\n\n# Generate initial and final shapes\ncircle &lt;- generate_circle(n)\nline &lt;- generate_line(n)\n\n# Morph between the shapes\nmorphed_shapes &lt;- morph_shapes(circle, line, steps)\n\n# Plot all shapes\nplots &lt;- lapply(morphed_shapes, function(shape) {\n  ggplot(shape, aes(x, y)) + geom_point() + xlim(-1.5, 1.5) + ylim(-1.5, 1.5)\n})\ndo.call(grid.arrange, plots)\n\n\n# # save out arranged plot\n# ggsave(filename = here::here(\"3. Analysis\",\"images\",\"001_cirlce_to_line.png\"), do.call(grid.arrange, plots), dpi = 300, height = 8, width = 8)\n\nFigure 1. Iterations of simulated annealing transition from Circle to Line\n\n\n\n\n\nYou can even go a step further and animate the process.\n\n# Create a data.frame to hold all morphed shapes with an additional 'frame' column\nall_frames &lt;- data.frame(x = numeric(), y = numeric(), frame = integer())\nfor (i in 1:length(morphed_shapes)) {\n  frame_data &lt;- cbind(morphed_shapes[[i]], frame = i)\n  all_frames &lt;- rbind(all_frames, frame_data)\n}\n\n# Create gganimate plot\np &lt;- ggplot(all_frames, aes(x, y)) +\n  geom_point() +\n  xlim(-1.5, 1.5) + ylim(-1.5, 1.5) +\n  transition_time(frame) +\n  ease_aes('linear')\n\n# Create the animation object\ng &lt;- gganimate::animate(p)\n\n# Save the animation\n# I found that the gifski_renderer renderer seems to work well for saving out gifs\n# anim_save(here::here(\"circle_to_line_animation.gif\"), animation = g, renderer = gifski_renderer)\n\nFigure 2. Simulated Annealing transition animation from Circle to Line\n\n\n\nResources\n\nI originally got the idea to work on this project from an updated version of Anscombe’s Quartet called the “Datasaurus Dozen”. It is a set of 12 distinct datasets, each sharing identical summary statistics but demonstrating vastly different shapes and patterns. It highlights the importance of data visualization in uncovering hidden insights that summary statistics alone may overlook. The animations in the Datasaurus Dozen are based off work using simulated annealing to generate the transitions (here is link to the paper)."
  },
  {
    "objectID": "posts/005_blog_cessie/index.html",
    "href": "posts/005_blog_cessie/index.html",
    "title": "Introduction to the Cessie-Houwelingen Test Statistic",
    "section": "",
    "text": "I first read about the Cessie-Houwelingen Test in section 10.5 Assessment of Model Fit from Dr. Frank Harrell’s “Regression Modeling Strategies, 2nd Ed.” years ago and have always wanted to do more of a deep dive into the inner workings of the test statistic.\nS. le Cessie and J. C. van Houwelingen in their 1991 article “A Goodness-of-Fit Test for Binary Regression Models, Based on Smoothing Methods” published in Biometrics proposed a global test statistic for logistic regression models. This test has a few other names: le Cessie-van Houwelingen Test, le Cessie-van Houwelingen Normality Test, le Cessie-van Houwelingen goodness of fit Test, and I’m sure a few others. For the sake of brevity, I will be sticking with the Cessie-Houwelingen Test for the rest of this article.\nNOTE: It’s important to distinguish the Cessie-Houwelingen Test from the Cessie-Houwelingen-Copas-Hosmer unweighted sum of squares test, which is a separate method for assessing global goodness of fit. This test, also referred to as the unweighted sum of squares test or the le Cessie-van Houwelingen normal test statistic for the unweighted sum of squared errors, is elaborated upon in the section discussing its historical context. Hosmer (1997) demonstrated through simulations the robustness of this test, and it has been recommended by Harrell for use. It wasn’t until writing this article that I realized they are two separate tests!\nThe Cessie-Houwelingen Test statistic is essentially a sum of squared standardized residuals post kernel smoothing. The kernel smoothing is particularly important as it accounts for the continuous attributes of covariates, enabling a more detailed and nuanced assessment of how well the model performs (as compared to binning the predictions like in the Hosmer-Lemeshow Test). The essence of this test statistic lies in integrating these refined, smoothed residuals to yield a comprehensive measure of the model’s overall fit.\nCalculation Steps\nThe Cessie-Houwelingen Test Statistic is calculated as follows:\n\nStandardization of Residuals\nKernel Smoothing of Residuals\nAdjusting Smooth Residuals by Inverse Variance\n\nLets break down each step:\n1. Standardization of Residuals\nFirst we need to compute the standardized residuals from the logistic regression model:\n\\[ r_i = \\frac{Y_i - \\pi(X_i)}{\\sqrt{\\pi(X_i)(1 - \\pi(X_i))}} \\]\nwhere \\(Y_{i}\\) are the observed responses, \\(\\pi(X_i)\\) are the predicted probabilities from the logistic model and \\(r_i\\) are the standardized residuals.\n\\(\\pi(X_i)\\) is the predicted outputs for the logistic model: \\(\\pi(x) = \\frac{e^{\\alpha + x\\beta}}{1 + e^{\\alpha + x\\beta}}\\)\nFor each observation, the residual is defined as the difference between the observed binary outcome \\(Y_i\\) and the predicted probability \\(\\pi(X_i)\\) from the logistic regression model. The difference is then standardized by dividing it by the standard deviation of the predicted probability: \\({\\sqrt{\\pi(X_i)(1 - \\pi(X_i))}}\\).\n2. Smoothing Residuals\n\\[\\hat{r}(x) = \\sum_{i=1}^n K_h(x - X_i) r_i\\]\nwhere \\(K_h\\) is the kernel function with bandwidth \\(h\\), and \\(\\hat{r}(x)\\) are the smoothed residuals. The specific smoothing function used in (Cessie & Houwelingen 1991) was the Nadaraya-Watson Estimator which is a weighted average of the residuals in the neighborhood of \\(x\\), where the bandwidth determines the size of the region over which the residuals are averaged and the kernel function determines the weighting. The specific kernel function used in the manuscript (for simplcity?) was the uniform kernel function.\nUniform Kernel Function: \\[ K(x) = \\begin{cases} \\frac{1}{2} & \\text{if } |x| \\leq 1, \\\\ 0 & \\text{otherwise.} \\end{cases}\\]\n3. Test Statistic\n\\[T = n \\sum_{i=1}^n \\hat{r}(X_i)^2 U(X_i)\\]\nwith \\(U(X_i)\\) being the inverse of the variance of the smoothed residual at \\(X_i\\) and \\(n\\) is the sample size.\nThe inverse of the variance is a measure of precision or reliability. It indicates how much we trust the estimate of the smoothed residual at each observation. When squared residuals are weighted by the inverse of their variances, observations with more precise estimates (i.e., lower variance) contribute more to the test statistic, while observations with less precise estimates (i.e., higher variance) contribute less.\nThe authors mention that each observation contributes to the test statistic \\(T\\) in a way that is proportional to the square of the predicted probability \\(p(X_i)\\) for that observation. Importantly, this contribution is proportional to a factor called \\(U(X_i)\\), which is the inverse of the variance of the smoothed standardized residual at observation \\(X_i\\). Thus each observation should contribute to the test statistic in a similar manner. The multiplication of \\(\\hat{r}(X_i)^2\\) by \\(U(X_i)\\) scales the contributions of each observation to account for the precision of the smoothed residuals at the observation. Observations with more precise smoothed residuals (lower variance) will have a larger value of \\(U(X_i)\\), which means their contribution is increased. In contrast observations with less precise smoothed residuals (higher variance) will have a smaller value of \\(U(X_i)\\) and their contribution is decreased.\nR Implementation\nI discovered a single R implementation of the Cessie-Houwelingen Test in the smwrStats package, which is now archived. This package was developed by Dr. Laura DeCicco at the United States Geological Survey (USGS). For continuous data, this implementation seems to compute the average of the residuals using Euclidean distance. In contrast, for categorical data, the residuals are calculated using Manhattan distance. Then it appears to use local smoothing instead of the Nadaraya-Watson Estimator. The chosen default binwidth is to utilize the mean of the distances as the binwidth when no specific width is specified. Here, I’ve developed a somewhat tidy version of the implementation:\n\nlibrary(\"tidyverse\")\nlibrary(\"purrr\")\n\n\nleCessie_test &lt;- function(object, bandwidth) {\n  # Define a function to calculate categorical distances\n  categorical &lt;- function(x) {\n    x &lt;- as.matrix(x)\n    retval &lt;- apply(x, 2, function(y) {\n      nrcats &lt;- length(unique(y))\n      if (nrcats == 1) {\n        disty &lt;- rep(0, length(y) * (length(y) - 1))\n      } else {\n        disty &lt;- as.numeric(dist(y, 'manhattan') != 0) * nrcats / (nrcats - 1)\n      }\n      return(disty)\n    })\n    return(rowSums(retval))\n  }\n\n  # Store the original object and construct the variable name\n  object.orig &lt;- object\n  Dname &lt;- as.character(object$call$formula)\n  Dname &lt;- paste(Dname[2], Dname[1], Dname[3]) # Arrange variable names in the correct order\n\n  # Calculate fitted values and residuals from the model\n  fits &lt;- fitted(object)\n  resids &lt;- resid(object, 'response')\n  N &lt;- length(resids)\n  \n  # Extract the model frame and check if response is a matrix\n  obj.frame &lt;- model.frame(object)\n  if (class(obj.frame[[1]]) == 'matrix') stop(\"Cannot perform test where response is matrix\")\n\n  # Calculate distances for each predictor in the model\n  distances &lt;- lapply(obj.frame[,-1, drop=FALSE], function(x) {\n    if (is.numeric(x)) {\n      scaled_x &lt;- scale(x)\n      dist_x &lt;- as.matrix(dist(scaled_x))\n      return(0.5 * dist_x * dist_x)  # Compute squared Euclidean distance\n    } else if (class(x) == 'factor') {\n      return(categorical(x))  # Use the categorical distance function for factor variables\n    } else {\n      scaled_x &lt;- scale(x[[1]])\n      dist_x &lt;- as.matrix(dist(scaled_x))\n      return(0.5 * dist_x * dist_x)  # Handle structured variables created from a function\n    }\n  })\n\n  # Initialize a matrix for combined distances and fill it\n  dist.mat &lt;- matrix(0, N, N)\n  for (dist_x in distances) {\n    dist.mat[lower.tri(dist.mat)] &lt;- dist.mat[lower.tri(dist.mat)] + sqrt(dist_x[lower.tri(dist_x)])\n  }\n  dist.mat &lt;- dist.mat + t(dist.mat)  # Symmetrize the distance matrix\n\n  # Set bandwidth if not specified\n  if (missing(bandwidth)) bandwidth &lt;- mean(dist.mat)\n  R.raw &lt;- pmax(1 - dist.mat / bandwidth, 0)  # Compute the raw smoother matrix\n\n  # Calculate smoothed residuals and raw Q\n  smoothres &lt;- t(resids) %*% R.raw\n  Q.raw &lt;- sum(smoothres * resids)\n\n  # Prepare matrices for the corrected estimation of Q\n  obj.mat &lt;- model.matrix(object.orig)\n  mu2 &lt;- fits * (1 - fits)\n  V &lt;- diag(mu2)\n  Vx &lt;- diag(V)\n  obj.hat &lt;- Vx * obj.mat %*% solve(t(obj.mat) %*% (Vx * obj.mat)) %*% t(obj.mat)\n  R.cor &lt;- (diag(N) - obj.hat) %*% R.raw %*% (diag(N) - obj.hat)\n\n  # Compute corrected estimated value of Q and its standard error\n  E.Q &lt;- sum(diag(R.cor) * mu2)\n  mu4 &lt;- mu2 * (1 - 3 * mu2)\n  VarQ1 &lt;- sum(diag(R.cor)^2 * (mu4 - 3 * mu2^2))\n  R.tmp &lt;- R.cor * matrix(rep(mu2, each = N), nrow = N)\n  VarQ2 &lt;- 2 * sum(diag(R.tmp %*% R.tmp))\n  VarQ &lt;- VarQ1 + VarQ2\n\n  # Calculate the test statistic and degrees of freedom\n  Test &lt;- Q.raw * 2 * E.Q / VarQ\n  df &lt;- 2 * E.Q^2 / VarQ\n\n  # Construct the output\n  output &lt;- data.frame(statistic = Test, \n             df = df,\n             p.value = 1 - pchisq(Test, df),\n             q = Q.raw, \n             e_q = E.Q,\n             se_q = sqrt(VarQ))\n  \n  return(output)\n  \n}\n\nThen testing it out on the titanic dataset.\n\ntitanic &lt;- titanic::titanic_train %&gt;% dplyr::select(Survived, Age) %&gt;% filter(!is.na(Age))\n\nglm_model &lt;- glm(Survived ~ Age, data = titanic)\n\n\n\n  statistic       df     p.value        q      e_q     se_q\n1  20.56367 5.171196 0.001133206 380.1192 95.58951 59.44697\n\n\nA p-value of 0.001133206 in the Cessie-Houwelingen Test strongly suggests that the model does not fit the data well. This result indicates a need to review and potentially revise the model to better capture the underlying relationship in the observed data.\nWhile there might not be alot of implenetations of the Cessie-Houwelingen Test, there are quite a few of the Cessie-Houwelingen-Copas-Hosmer unweighted sum of squares test:\n\nrms::residuals(rms::lrm(Y ~ X, data = simulated_data, x=TRUE, y=TRUE), type = \"gof\")\nDescTools::HosmerLemeshowTest(fit)\nMKmisc::HLgofTest(fit)\n\nA simple explanation of the Cessie-Houwelingen\nThus you can consider the Cessie-Houlingen Test as a comparison of the observed values to the standardized residuals after weighting them by the inverse of the variance of a kernel smoothing function.\n\n\n\nSome Historical Context\nI believed it would be beneficial to provide a succinct overview of the timeline regarding the development of the Cessie & Houwelingen Statistic in relation to other global goodness-of-fit tests for logistic regression. Please note, this is not a comprehensive historical chronology.\n\n1980 - Hosmer and Lemeshow introduced a widely-used method for assessing the goodness-of-fit in logistic regression models. This technique involves categorizing data into several groups based on the model’s predicted probabilities and then computing a chi-squared-like statistic.\n1982 - Following this, Brown detailed a goodness-of-fit test for the logistic model based on score statistics. Brown’s approach extends the logistic model to a broader family of models with two additional parameters that, when set to zero, reduce the model to its standard logistic form.\n1983 - Copas later used kernel methods to visually assess the fit of these models.\n1989 - Copas introduced the unweighted sum of squares test for proportions.\n1991 - Cessie & Houwelingen - Modified Copas (1983) to create a weighted sum of squares based on nonparametric kernel methods.\n1995 - Expansion of the Cessie & Houwelingen Test Statistics to allow for unweighted sum of sqaures. - *Note that the unweighted sum of squares test (from Copas 1989) is considered a special case of the series of goodness-of-fit test statistics considered by Cessie and Houwelingen.\n1997 - Hosmer and colleagues compared various goodness-of-fit tests, including the Pearson Chi-Square, Unweighted Sum of Squares (Copas 1989), Hosmer-Lemeshow (C, H), Kernel Smoothing (Uniform and Cubic weighting, Cessie & Houwelingen 1991), Royston (monotone and quadratic), and the Stukel Score Test. They found the unweighted sum of squares was both simple and powerful.\n\nSome thoughts\n\nChanging Kernel Function and Bandwidth\n\nIn Section 7. Practical Considerations, the authors state:\n\n\n“In the literature of kernel regression and density estimation, it is suggested that the choice of the kernel function is not so important. The simplest kernel to deal with is the uniform kernel on \\([\\frac{1}{2},\\frac{1}{2}]\\), which has been used throughout his paper. There is no evidence that a different type of kernel function would alter the result significantly.”\n\n\nAlthough I believe this statement to be correct, I noticed that the authors haven’t provided any references to back it up. I’m curious to explore how the test statistic varies with the use of different kernel regressions, specifically Gaussian and Epanechnikov, and also how it’s affected by adjusting the bandwidth of the kernel function.\nGaussian Kernel Function\n\n\\(K(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} x^2}\\)\n\nEpanechnikov Kernel Function\n\n\\(K(x) = \\begin{cases} \\frac{3}{4}(1 - x^2) & \\text{if } |x| \\leq 1, \\\\ 0 & \\text{otherwise.} \\end{cases}\\)\n\n\nIt would be nice to recreate the results of the Hosmer 1997 paper to show the performance of the Cessie-Houwelingen-Copas-Hosmer unweighted sum of squares test.\n\nFurther Reading:\n\nHosmer, D.W., Hosmer, T. and Lemeshow, S. (1980) A Goodness-of-Fit Tests for the Multiple Logistic Regression Model. Communications in Statistics, 10, 1043-1069. https://doi.org/10.1080/03610928008827941\nCharles C. Brown (1982) On a goodness of fit test for the logistic model based on score statistics, Communications in Statistics - Theory and Methods, 11:10, 1087-1105, DOI: 10.1080/03610928208828295\nCopas, J. B. (1983). Plotting p against x. Journal of the Royal Statistical Society. Series C (Applied Statistics), 32(1), 25–31. https://doi.org/10.2307/2348040\nCopas, J. B. (1989). Unweighted Sum of Squares Test for Proportions. Journal of the Royal Statistical Society. Series C (Applied Statistics), 38(1), 71–80. https://doi.org/10.2307/2347682\nle Cessie, S., & Houwelingen, J.C. (1991). A goodness-of-fit test for binary regression models, based on smoothing methods. Biometrics, 47, 1267-1282.\nle Cessie, S., & van Houwelingen, H. C. (1995). Testing the fit of a regression model via score tests in random effects models. Biometrics, 51(2), 600–614.\nHosmer, D. W., Hosmer, T., Le Cessie, S., & Lemeshow, S. (1997). A comparison of goodness-of-fit tests for the logistic regression model. Statistics in medicine, 16(9), 965–980. https://doi.org/10.1002/(sici)1097-0258(19970515)16:9&lt;965::aid-sim509&gt;3.0.co;2-o\nHarrell, F. E., Jr. (2016). Regression modeling strategies. Springer International Publishing. 2nd Ed. Section 10.5 Aessment of Model Fit, p. 236."
  }
]