[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi, I’m Donald\nSince 2017, I’ve held the position of Biostatistician at IDEXX Laboratories within the Medical Data Insights team. My responsibilities predominantly revolve around three main domains:\n\nClinical Studies\nReal World Evidence (RWE) Studies\nValidation Studies for machine learning algorithms\n\nAs part of my work, I collaborate with over 40 universities and research institutions across 20 different countries:\n\nPreviously, I worked at Harvard Innovation Labs as a biostatistician consultant and Harvard Medical School/Beth Isreal Deaconess Medical Center as a biostatistician focused on large cardiovascular clinical trials.\nI received a Bachelor’s of Science in Human Biology with minors in Biochemistry and Physics from the University of Southern Maine. The focus of my undergraduate work was on the effects of heavy metals (arsenic) on the developing cytoskeleton of rat pheochromocytoma cells as a model for brain development.\nIn addition, I obtained both an undergraduate and graduate research fellowship from NASA focused on the effects of space radiation on developing glial cells and the mitigation of stunted neurite outgrowth through the use of antioxidants to mitigate damage from reactive oxygen species at the Biomedical Research and Operations branch of Johnson Space Center.\nMy graduate research focused on the implementation of a a clinical decision support system for traumatic brain injury (the Canadian Head CT scan rule) in a single large hospital and the evaluation of the performance of that implementation into an EHR."
  },
  {
    "objectID": "posts/002_blog_variance-sum-law/index.html",
    "href": "posts/002_blog_variance-sum-law/index.html",
    "title": "Variance Sum Law through R",
    "section": "",
    "text": "The variance sum law states that the expectation value of the sum of two independently random variables (\\(x\\) and \\(y\\)) equal the sum of the expectation values of the two variables:\n\\[\\begin{align}\n    var(x+y) = var(x)+var(y)\n\\end{align}\\]\nTo try and code this in R you might start using the stats::rnorm() function like so:\n\n\nx <- stats::rnorm(100)\n\ny <- stats::rnorm(100)\n\nvar(x) + var(y)\n## [1] 2.161509\n\nvar(x + y)\n## [1] 2.109248\n\nWhat is going on here? It does not look as though the variance sum law works in the case shown above. Is it possible that these two variables are not completely independent from one another?\nLet’s take a look:\n\n\ncor.test(x,y)\n## \n##  Pearson's product-moment correlation\n## \n## data:  x and y\n## t = -0.24075, df = 98, p-value = 0.8102\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.2196817  0.1729313\n## sample estimates:\n##         cor \n## -0.02431262\n\nThey are correlated! Because we did not ensure that the two variables are completely random with respect to one another, they are slightly correlated with each other.\nLets play around with this a little. How much correlation we would expect between two random variables? Why don’t we simulate a distribution of random variables and see how correlated these to random variables are:\n\nset.seed(123)\n\nvector_list <- 1:10000\n\ncorr_data <- data.frame()\nfor (variable in vector_list) {\n  \nx <- stats::rnorm(100)\n\ny <- stats::rnorm(100)\n\ncor_est <- cor.test(x,y)[4]\n\ncorr_data <- rbind(corr_data,cor_est)\n  \n}\n\nggplot(corr_data, aes(x = estimate)) +\n       geom_histogram(bins = 100, color = \"black\", fill = \"lightgrey\") +\n       scale_x_continuous(\"Correlation Estimate\", expand = c(0,0)) +\n       scale_y_continuous(\"Frequency\", expand = c(0,0))\n\n\n\n\n\nround(max(corr_data$estimate),2)\n## [1] 0.36\n\nround(min(corr_data$estimate),2)\n## [1] -0.41\n\nWhile the 10,000 simulated values appear to be centered around zero, there is quite a large spread in the results with correlation coefficient as low as -0.41 and as high as 0.36!\nNow that we know the reason the variance sum law doesn’t seem to work in this case, how can we generate two random variables two with correlation between them?\nWe can start by taking a look at the differences in the correlation of these values using a least squares regression model and exploring the residuals.\n\n\nset.seed(666) #set seed rock on \\m/\ny <- rnorm(10)\n\nx <- rnorm(10)\n\nlm <- lm(y ~ x)\n\n#show lm coefficients\nlm$coefficients\n## (Intercept)           x \n## -0.04818839 -0.48394162\n\n# pull out the slope of the regression model\nslope <- lm$coefficients[2]\n\n# pull out the y-intercept of the regression model\nintercept <- lm$coefficients[1]\n\n#get the predicted valus from the regression model\nyhat <- lm$fitted.values\n\n# difference between predicted values and actual (residuals)\ndiff_yx <- y-yhat\n\n# plot data - qplot allows the use of vectors over data frames\nqplot(x=x, y=y)+\n      # plot regression slope\n      geom_abline(slope = slope, intercept = intercept)  +\n      # add the residuals\n      geom_segment(aes(x=x, xend=x, y=y, yend=yhat), color = \"red\", linetype = \"dashed\") +\n      # plot points \"pch\" just allows me to fill in the points\n      geom_point(fill=\"white\",colour=\"black\",pch=21)\n\n\n\n\n\n # plot table of data points, predicted values (yhat) and residuals (diff_yx)\nknitr::kable(data.frame(x,y,yhat,diff_yx))\n\n\n\n\nx\ny\nyhat\ndiff_yx\n\n\n\n\n2.1500426\n0.7533110\n-1.0886835\n1.8419945\n\n\n-1.7702308\n2.0143547\n0.8085000\n1.2058547\n\n\n0.8646536\n-0.3551345\n-0.4666303\n0.1114958\n\n\n-1.7201559\n2.0281678\n0.7842666\n1.2439012\n\n\n0.1341257\n-2.2168745\n-0.1130974\n-2.1037771\n\n\n-0.0758266\n0.7583962\n-0.0114928\n0.7698889\n\n\n0.8583005\n-1.3061853\n-0.4635557\n-0.8426295\n\n\n0.3449003\n-0.8025196\n-0.2151000\n-0.5874195\n\n\n-0.5824527\n-1.7922408\n0.2336847\n-2.0259255\n\n\n0.7861704\n-0.0420325\n-0.4286490\n0.3866165\n\n\n\n\n\nIn the least square regression of \\(x\\) against \\(y\\), the residuals represent the removal of the \\(y\\) component from \\(x\\), giving a column of values that are orthogonal (i.e. at right angles) to the values of \\(y\\). We can add back in a multiple of \\(y\\) that will give us a vector of values with our desired correlation. Since we are looking for a correlation of zero, the \\(y\\) component that we are adding back in is the multiple the standard deviation and the residual around our \\(y\\) value.\n\n\n# the residual of x against y (not to be confused with y against X from above)\ndiff_xy <- residuals(lm(x~y))\n\n\n# our new x value which is the is the multiple the standard deviation and the residual around our y value\nx2 <- diff_xy*sd(y)\n\n\n# correlation between our y value and our new x value\ncor(y,x2)\n## [1] -4.778814e-17\n\n\n#round to 5 digits\nround(cor(y,x2),5)\n## [1] 0\n\nWoohoo! Now that we have our new random variable (\\(x_2\\)) with no correlation against our \\(y\\) values. Lets try to run the variance sum law again.\n\n\nvar(x2) + var(y)\n## [1] 4.930332\n\nvar(x2 + y)\n## [1] 4.930332\n\nAwesome! Now we have been able to show that the expectation value of the sum of two independently random variables (\\(x\\) and \\(y\\)) equal the sum of the expectation values of the two variables!\nAnd just for the heck of it, lets turn that code into a function for future use!\n\n\n\n# functionalizing the code above\nno_corr_variable <- function(y, x) {\n  diff_xy <- residuals(lm(x ~ y))\n  diff_xy * sd(y)\n}\n\n\nSupplementary: Expansions to other Correlations\nThere has actually been a lot of work done around expanding this problem for different distributions and correlations. In fact, the simple program we wrote for a correlation of zero was simplified from a more generalized equation. A discussion around the generalization and expansion of this problem can be found on CrossValidated.\nThe generalized form:\n\\(X_{Y;\\rho} = \\rho SD (Y^{\\perp})Y + \\sqrt{1- \\rho^2}SD(Y)Y^{\\perp}\\),\nwhere vector \\(X\\) and vector \\(Y\\) have the same length, \\(Y^{\\perp}\\) is the residuals of the least squares regression of \\(X\\) against \\(Y\\), \\(\\rho\\) is the desired correlation, and \\(SD\\) stands for any calculation proportional to a standard deviation.\nSince we were looking for a situation in which \\(\\rho = 0\\) the above equation can be simplified as follows:\n\\(X_{Y;\\rho=0} = (0) \\cdot SD (Y^{\\perp})Y + \\sqrt{1- (0)^2}SD(Y)Y^{\\perp}\\)\n\\(X_{Y;\\rho=0} = \\sqrt{1}SD(Y)Y^{\\perp}\\)\n\\(X_{Y;\\rho = 0} = SD(Y)Y^{\\perp}\\)"
  },
  {
    "objectID": "posts/003_blog_tidymodel-mixed-effects/index.html",
    "href": "posts/003_blog_tidymodel-mixed-effects/index.html",
    "title": "A basic tutorial on plotting mixed effects using tidymodels",
    "section": "",
    "text": "In R there are a ton of packages available to regression models including mixed effects model but one of the biggest issues is the vast difference in syntax needed for each of the modeling packages. Tidymodels was developed to solve this problem with the goal of having similar syntax style to the other tidyverse packages. Tidymodels itself is a “meta-package” consisting of a bunch {https://tidymodels.tidymodels.org/} of packages for modeling and statistical analysis with a focus on using the design philosophy of the tidyverse packages.\nOne of the current packages in development (as of this blog post) is the multilevelmod package for hierarchical modeling.\nIn this tutorial I am going to go through how to create a mixed effects model in R using the tidymodels and multilevelmod packages and how to plot the random intercepts using ggplot2. This blog post is just focused on using tidymodels and is not an indept overview of what mixed effects models are or how to use them.\nFirst lets load our packages of interest. For the multilevelmod package (as of the time of this blog post) you will need to install it through github using devtools::install_github(). I ended up running into a little bit of a problem with an out of date Rcpp package. Deleting the folder manually and then re-installing ended up doig the trick. We will also be loading in one of R’s most used mixed effects modeling packages,lme4, to get some data.\n\n#install.packages(\"pacman\")\n#load required packages\npacman::p_load(\"tidyverse\",\"lme4\")\n\n# install multilevelmod\n#devtools::install_github(\"tidymodels/multilevelmod\")\n\nlibrary(\"multilevelmod\")\n\nLoading required package: parsnip\n\n\nNow lets load in the sleep study dataset from the lme4 package. I am also going to create a fake categorical variable to use as a fixed effect in the model.\n\n# load sleep study and create a fake category\nset.seed(666) #\\m/ rock on\nsleepstudy <- lme4::sleepstudy %>% group_by(Subject) %>% \n              # creating a new random category with 3 levels to explore group effects\n              mutate(cat = sample( LETTERS[1:3], 1, replace=TRUE, prob=c(0.25, 0.50, 0.25))) %>%\n              ungroup()\n\nThe Tidymodels syntax requires that we set the “engine” for the type of model that we want to use. It is kind of like picking the type of car to use in MarioKart. You want to use the right engine for the right type of data. Since we are interested in looking at repeated measures data using a mixed effects model we will be using the “lmer” engine. We can set up the engine with the following code:\n\n# set engine for mixed effects models\nmixed_model_spec <- linear_reg() %>% set_engine(\"lmer\")\n\nNext we can build the model using Reaction as our dependent variable, Days and cat as our fixed effects and Subject as our random effect. The random effect syntax following that of the lme4 package using (1|Subject) to define Subject as the random intercept.\n\n# create model\nmixed_model_fit_tidy <- mixed_model_spec %>% fit(Reaction ~ Days + cat + (1 | Subject), data = sleepstudy)\n\nmixed_model_fit_tidy\n\nparsnip model object\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ Days + cat + (1 | Subject)\n   Data: data\nREML criterion at convergence: 1769.298\nRandom effects:\n Groups   Name        Std.Dev.\n Subject  (Intercept) 39.58   \n Residual             30.99   \nNumber of obs: 180, groups:  Subject, 18\nFixed Effects:\n(Intercept)         Days         catB         catC  \n    248.683       10.467        3.407       11.516  \n\n\nNow that we have created our model. Lets take a look at the predicted probabilities. To do this, we will create a data frame with all the different combinations of our fixed and random effects.\n\nexpanded_df_tidy <- with(sleepstudy,\n                    data.frame(\n                      expand.grid(Subject=levels(Subject),\n                                  #cat = unique(cat),\n                                  Days=seq(min(Days),max(Days),length=51))))\n\nexpanded_df_tidy <- sleepstudy %>% tidyr::expand(Subject,Days,cat)\n\nexpanded_df_tidy\n\n# A tibble: 540 × 3\n   Subject  Days cat  \n   <fct>   <dbl> <chr>\n 1 308         0 A    \n 2 308         0 B    \n 3 308         0 C    \n 4 308         1 A    \n 5 308         1 B    \n 6 308         1 C    \n 7 308         2 A    \n 8 308         2 B    \n 9 308         2 C    \n10 308         3 A    \n# … with 530 more rows\n\n\nWe can use this data frame and the predict() to get the predictions from our model.\n\npredicted_df_tidy <- mutate(expanded_df_tidy,\n                            pred = predict(mixed_model_fit_tidy,\n                                           new_data=expanded_df_tidy, \n                                           type = \"raw\", opts=list(re.form=NA)))\n\n\npredicted_df_tidy\n\n# A tibble: 540 × 4\n   Subject  Days cat    pred\n   <fct>   <dbl> <chr> <dbl>\n 1 308         0 A      249.\n 2 308         0 B      252.\n 3 308         0 C      260.\n 4 308         1 A      259.\n 5 308         1 B      263.\n 6 308         1 C      271.\n 7 308         2 A      270.\n 8 308         2 B      273.\n 9 308         2 C      281.\n10 308         3 A      280.\n# … with 530 more rows\n\n\nWhen looking at the prediction output, notice that we are getting the same predictions for each subject. The predict function is currently giving us predictions for the fixed effects. If were were to run this same code using predict() with lme4 we would get the predictions for the random effects for each `Subject.\nWhat is going on here? The issue is that multilevelmod package internally sets the default for prediction to re.form = NA;. In lme4 the default for predictions is re.form = NULL (i.e. include all random effects in the prediction).\n\nknitr::include_graphics(\"tidymodel_git_comment.PNG\")\n\n\n\n\nWe can include re.form = NULL in the predict() function by using the opts argument.\n\n#update predictions\npredicted_df_tidy <- mutate(expanded_df_tidy,\n                            # get random predictions\n                            pred_rand = predict(mixed_model_fit_tidy,\n                                                new_data=expanded_df_tidy, \n                                                type = \"raw\", opts=list(re.form=NULL)),\n                            # get fixed effect predictions\n                            pred_fixed = predict(mixed_model_fit_tidy,\n                                                new_data=expanded_df_tidy, \n                                                type = \"raw\", opts=list(re.form=NA)))\n\npredicted_df_tidy\n\n# A tibble: 540 × 5\n   Subject  Days cat   pred_rand pred_fixed\n   <fct>   <dbl> <chr>     <dbl>      <dbl>\n 1 308         0 A          292.       249.\n 2 308         0 B          296.       252.\n 3 308         0 C          304.       260.\n 4 308         1 A          303.       259.\n 5 308         1 B          306.       263.\n 6 308         1 C          314.       271.\n 7 308         2 A          313.       270.\n 8 308         2 B          317.       273.\n 9 308         2 C          325.       281.\n10 308         3 A          324.       280.\n# … with 530 more rows\n\n\nNow that we have both the predictions for the fixed and random effects we can plot them using ggplot2!\n\nggplot(predicted_df_tidy) +\n       facet_wrap(.~cat) + \n       geom_line(aes(x=Days,y=pred_fixed), size = 1) + \n       geom_line(aes(x=Days,y=pred_rand,colour=Subject)) +\n       scale_y_continuous(\"Predicted\") + \n       theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Hello, World!\nWelcome to my blog!\nWhile this blog will be mainly focused on statistics, I am (at least hoping) to have this blog broken up into a few sections!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Introduction to Simulated Annealing in R\n\n\n\n\n\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\nDonald Szlosek\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2023\n\n\nDonald Szlosek\n\n\n\n\n\n\n  \n\n\n\n\nA basic tutorial on plotting mixed effects using tidymodels\n\n\n\n\n\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2020\n\n\nDonald Szlosek\n\n\n\n\n\n\n  \n\n\n\n\nVariance Sum Law through R\n\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2020\n\n\nDonald Szlosek\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Rarely do we see linear relationships in nature, yet a lot of research is modeled using linear models. My research focuses on the use of non-linear models to better understand the complex relationship between between biomarkers and disease.\nIn addition, I also focus on the development and application of improved methods for handling of imperfect hold standards in the training and validation of machine learning algorithms, with the goal of building more robust medical AI systems."
  },
  {
    "objectID": "posts/004_blog_intro_simulated_annealing/index.html",
    "href": "posts/004_blog_intro_simulated_annealing/index.html",
    "title": "Introduction to Simulated Annealing in R",
    "section": "",
    "text": "Simulated annealing is a stochastic optimization algorithm that draws inspiration from the annealing process in metallurgy, where metals are heated and slowly cooled to achieve a low-energy crystalline state. The goal is to find an optimal solution to a problem by exploring a solution space and “cooling down” over time. Here’s a mathematical representation of the key components and the intuition behind simulated annealing:\n\nObjective Function: In optimization problems, we have an objective function \\(f(x)\\) that assigns a value to each candidate solution \\(x\\). The objective is to either maximize or minimize this function.\nSolution Space: The set of all possible solutions to the problem is called the solution space, denoted as \\(X\\). Each point \\(x\\) in this space corresponds to a specific solution.\nEnergy Landscape: The objective function defines an “energy landscape” over the solution space. Higher values of the objective function correspond to higher energy states, and lower values correspond to lower energy states. In some cases, the landscape may have multiple local optima, making it challenging to find the global optimum.\nTemperature: Simulated annealing introduces the concept of temperature (\\(T\\)), which is initially set to a high value. The temperature controls the exploration versus exploitation trade-off. Higher temperatures allow for more exploration (acceptance of worse solutions), while lower temperatures focus on exploitation (acceptance of better solutions).\nNeighbor Generation: To explore the solution space, we generate neighboring solutions from the current solution. This is done using a “neighbor generation” function, \\(x' = \\text{generate\\_neighbor}(x)\\), which perturbs the current solution.\nAcceptance Probability: Simulated annealing accepts or rejects neighboring solutions based on a probability distribution. The probability of accepting a worse solution (\\(x'\\)) is determined by:\n\n\\[\nP(\\text{accept}) = \\begin{cases}\n1, & \\text{if } f(x') &lt; f(x) \\quad \\text{(worse solution)} \\\\\ne^{-\\frac{f(x') - f(x)}{T}}, & \\text{if } f(x') \\geq f(x) \\quad \\text{(better solution)}\n\\end{cases}\n\\]\nHere, \\(e\\) is the base of the natural logarithm, and \\(T\\) is the current temperature. When \\(T\\) is high, the algorithm is more likely to accept worse solutions, enabling exploration. As \\(T\\) decreases, it becomes less likely to accept worse solutions, focusing on exploitation.\n\nCooling Schedule: The temperature is reduced gradually over time according to a “cooling schedule.” A common choice is to decrease the temperature exponentially:\n\n\\[\nT_{\\text{new}} = \\alpha \\cdot T_{\\text{old}}\n\\]\nHere, \\(\\alpha\\) is the cooling rate (typically between 0 and 1).\n\nTermination: The algorithm iterates through these steps for a specified number of iterations or until a termination condition is met (e.g., a low temperature).\nExploration History: Throughout the process, the algorithm maintains a record of the solutions explored. This exploration history can be visualized to understand how the algorithm traversed the solution space.\n\nBy combining these components, simulated annealing gradually explores the solution space, initially accepting worse solutions with a higher probability and later focusing on exploiting better solutions as the temperature decreases. This stochastic exploration allows it to escape local optima and converge to a near-optimal solution.\nYou can use this simulated annealing to morphs between shapes, by defining The sequence of shapes symbolizes the solution space. Unlike traditional acceptance probabilities, this code uniformly generates and visualizes intermediate shapes for a comprehensive exploration. Although there’s no cooling schedule, the gradual and continuous transitions mimic the annealing process. The more steps you allow, the “smoother” the transition (althorugh the time to run the code gets much longer).\nHere is some code that transitions from a circle to a line:\n\n# load dependencies\n#install.packages(\"pacman\")\npacman::p_load(\"ggplot2\",\"gridExtra\",\"ggplot2\",\"gganimate\",\"transformr\",\"gifski\")\n\n\n# Function to generate circle points\ngenerate_circle &lt;- function(n) {\n  theta &lt;- seq(0, 2 * pi, length.out = n + 1)[1:n]\n  x &lt;- cos(theta)\n  y &lt;- sin(theta)\n  data.frame(x, y)\n}\n\n# Function to generate horizontal line points\ngenerate_line &lt;- function(n) {\n  x &lt;- seq(-1, 1, length.out = n)\n  y &lt;- rep(0, n)\n  data.frame(x, y)\n}\n\n# Function to morph between two shapes\nmorph_shapes &lt;- function(shape1, shape2, steps) {\n  morphed_shapes &lt;- vector(\"list\", steps)\n  for (i in 1:steps) {\n    alpha &lt;- (i - 1) / (steps - 1)\n    morphed_shapes[[i]] &lt;- shape1 * (1 - alpha) + shape2 * alpha\n  }\n  morphed_shapes\n}\n\n# Number of points and steps\nn &lt;- 50\nsteps &lt;- 20\n\n# Generate initial and final shapes\ncircle &lt;- generate_circle(n)\nline &lt;- generate_line(n)\n\n# Morph between the shapes\nmorphed_shapes &lt;- morph_shapes(circle, line, steps)\n\n# Plot all shapes\nplots &lt;- lapply(morphed_shapes, function(shape) {\n  ggplot(shape, aes(x, y)) + geom_point() + xlim(-1.5, 1.5) + ylim(-1.5, 1.5)\n})\ndo.call(grid.arrange, plots)\n\n\n# # save out arranged plot\n# ggsave(filename = here::here(\"3. Analysis\",\"images\",\"001_cirlce_to_line.png\"), do.call(grid.arrange, plots), dpi = 300, height = 8, width = 8)\n\nFigure 1. Iterations of simulated annealing transition from Circle to Line\n\n\n\n\n\nYou can even go a step further and animate the process.\n\n# Create a data.frame to hold all morphed shapes with an additional 'frame' column\nall_frames &lt;- data.frame(x = numeric(), y = numeric(), frame = integer())\nfor (i in 1:length(morphed_shapes)) {\n  frame_data &lt;- cbind(morphed_shapes[[i]], frame = i)\n  all_frames &lt;- rbind(all_frames, frame_data)\n}\n\n# Create gganimate plot\np &lt;- ggplot(all_frames, aes(x, y)) +\n  geom_point() +\n  xlim(-1.5, 1.5) + ylim(-1.5, 1.5) +\n  transition_time(frame) +\n  ease_aes('linear')\n\n# Create the animation object\ng &lt;- gganimate::animate(p)\n\n# Save the animation\n# I found that the gifski_renderer renderer seems to work well for saving out gifs\n# anim_save(here::here(\"circle_to_line_animation.gif\"), animation = g, renderer = gifski_renderer)\n\nFigure 2. Simulated Annealing transition animation from Circle to Line\n\n\n\nResources\n\nI originally got the idea to work on this project from an updated version of Anscombe’s Quartet called the “Datasaurus Dozen”. It is a set of 12 distinct datasets, each sharing identical summary statistics but demonstrating vastly different shapes and patterns. It highlights the importance of data visualization in uncovering hidden insights that summary statistics alone may overlook. The animations in the Datasaurus Dozen are based off work using simulated annealing to generate the transitions (here is link to the paper)."
  }
]