{
  "hash": "fa0b3bbf25ad040711d33cc734d2a829",
  "result": {
    "markdown": "---\ntitle: \"Variance Sum Law through R\"\nauthor: \"Donald Szlosek\"\ndate: 2020-05-06T21:19:39-05:00\ncategories: [\"R\",\"statistics\"]\ntags: [\"R\",\"statistics\",\"variance\"]\n---\n\n\n\n\nThe variance sum law states that the expectation value of the sum of two independently random variables ($x$ and $y$) equal the sum of the expectation values of the two variables:\n\n\n```{=tex}\n\\begin{align}\n    var(x+y) = var(x)+var(y)\n\\end{align}\n```\n\nTo try and code this in R you might start using the `stats::rnorm()` function like so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nx <- stats::rnorm(100)\n\ny <- stats::rnorm(100)\n\nvar(x) + var(y)\n## [1] 2.203226\n\nvar(x + y)\n## [1] 2.21042\n```\n:::\n\n\nWhat is going on here? It does not look as though the variance sum law works in the case shown above. Is it possible that these two variables are not completely independent from one another?\n\nLet's take a look:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\ncor.test(x,y)\n## \n## \tPearson's product-moment correlation\n## \n## data:  x and y\n## t = 0.032426, df = 98, p-value = 0.9742\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.1932669  0.1995653\n## sample estimates:\n##         cor \n## 0.003275545\n```\n:::\n\n\nThey are correlated! Because we did not ensure that the two variables are completely random with *respect to one another*, they are slightly correlated with each other.\n\nLets play around with this a little. How much correlation we would expect between two random variables? Why don't we simulate a distribution of random variables and see how correlated these to random variables are:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\nvector_list <- 1:10000\n\ncorr_data <- data.frame()\nfor (variable in vector_list) {\n  \nx <- stats::rnorm(100)\n\ny <- stats::rnorm(100)\n\ncor_est <- cor.test(x,y)[4]\n\ncorr_data <- rbind(corr_data,cor_est)\n  \n}\n\nggplot(corr_data, aes(x = estimate)) +\n       geom_histogram(bins = 100, color = \"black\", fill = \"lightgrey\") +\n       scale_x_continuous(\"Correlation Estimate\", expand = c(0,0)) +\n       scale_y_continuous(\"Frequency\", expand = c(0,0))\n```\n\n::: {.cell-output-display}\n![](002-blog_variance-sum-law_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n\nround(max(corr_data$estimate),2)\n## [1] 0.36\n\nround(min(corr_data$estimate),2)\n## [1] -0.41\n```\n:::\n\n\nWhile the 10,000 simulated values appear to be centered around zero, there is quite a large spread in the results with correlation coefficient as low as -0.41 and as high as 0.36!\n\nNow that we know the reason the variance sum law doesn't seem to work in this case, how can we generate two random variables two with correlation between them?\n\nWe can start by taking a look at the differences in the correlation of these values using a least squares regression model and exploring the residuals.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nset.seed(666) #set seed rock on \\m/\ny <- rnorm(10)\n\nx <- rnorm(10)\n\nlm <- lm(y ~ x)\n\n#show lm coefficients\nlm$coefficients\n## (Intercept)           x \n## -0.04818839 -0.48394162\n\n# pull out the slope of the regression model\nslope <- lm$coefficients[2]\n\n# pull out the y-intercept of the regression model\nintercept <- lm$coefficients[1]\n\n#get the predicted valus from the regression model\nyhat <- lm$fitted.values\n\n# difference between predicted values and actual (residuals)\ndiff_yx <- y-yhat\n\n# plot data - qplot allows the use of vectors over data frames\nqplot(x=x, y=y)+\n      # plot regression slope\n      geom_abline(slope = slope, intercept = intercept)  +\n      # add the residuals\n      geom_segment(aes(x=x, xend=x, y=y, yend=yhat), color = \"red\", linetype = \"dashed\") +\n      # plot points \"pch\" just allows me to fill in the points\n      geom_point(fill=\"white\",colour=\"black\",pch=21)\n```\n\n::: {.cell-output-display}\n![](002-blog_variance-sum-law_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n # plot table of data points, predicted values (yhat) and residuals (diff_yx)\nknitr::kable(data.frame(x,y,yhat,diff_yx))\n```\n\n::: {.cell-output-display}\n|          x|          y|       yhat|    diff_yx|\n|----------:|----------:|----------:|----------:|\n|  2.1500426|  0.7533110| -1.0886835|  1.8419945|\n| -1.7702308|  2.0143547|  0.8085000|  1.2058547|\n|  0.8646536| -0.3551345| -0.4666303|  0.1114958|\n| -1.7201559|  2.0281678|  0.7842666|  1.2439012|\n|  0.1341257| -2.2168745| -0.1130974| -2.1037771|\n| -0.0758266|  0.7583962| -0.0114928|  0.7698889|\n|  0.8583005| -1.3061853| -0.4635557| -0.8426295|\n|  0.3449003| -0.8025196| -0.2151000| -0.5874195|\n| -0.5824527| -1.7922408|  0.2336847| -2.0259255|\n|  0.7861704| -0.0420325| -0.4286490|  0.3866165|\n:::\n:::\n\n\nIn the least square regression of $x$ against $y$, the residuals represent the removal of the $y$ component from $x$, giving a column of values that are orthogonal (i.e. at right angles) to the values of $y$. We can add back in a multiple of $y$ that will give us a vector of values with our desired correlation. Since we are looking for a correlation of zero, the $y$ component that we are adding back in is the multiple the standard deviation and the residual around our $y$ value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n# the residual of x against y (not to be confused with y against X from above)\ndiff_xy <- residuals(lm(x~y))\n\n\n# our new x value which is the is the multiple the standard deviation and the residual around our y value\nx2 <- diff_xy*sd(y)\n\n\n# correlation between our y value and our new x value\ncor(y,x2)\n## [1] -4.778814e-17\n\n\n#round to 5 digits\nround(cor(y,x2),5)\n## [1] 0\n```\n:::\n\n\nWoohoo! Now that we have our new random variable ($x_2$) with no correlation against our $y$ values. Lets try to run the variance sum law again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nvar(x2) + var(y)\n## [1] 4.930332\n\nvar(x2 + y)\n## [1] 4.930332\n```\n:::\n\n\nAwesome! Now we have been able to show that the expectation value of the sum of two independently random variables ($x$ and $y$) equal the sum of the expectation values of the two variables!\n\nAnd just for the heck of it, lets turn that code into a function for future use!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n\n# functionalizing the code above\nno_corr_variable <- function(y, x) {\n  diff_xy <- residuals(lm(x ~ y))\n  diff_xy * sd(y)\n}\n```\n:::\n\n\n#### Supplementary: Expansions to other Correlations\n\nThere has actually been a lot of work done around expanding this problem for different distributions and correlations. In fact, the simple program we wrote for a correlation of zero was simplified from a more generalized equation. A discussion around the generalization and expansion of this problem can be found on [CrossValidated](https://stats.stackexchange.com/questions/15011/generate-a-random-variable-with-a-defined-correlation-to-an-existing-variables).\n\nThe generalized form:\n\n$X_{Y;\\rho} = \\rho SD (Y^{\\perp})Y + \\sqrt{1- \\rho^2}SD(Y)Y^{\\perp}$,\n\nwhere vector $X$ and vector $Y$ have the same length, $Y^{\\perp}$ is the residuals of the least squares regression of $X$ against $Y$, $\\rho$ is the desired correlation, and $SD$ stands for any calculation proportional to a standard deviation.\n\nSince we were looking for a situation in which $\\rho = 0$ the above equation can be simplified as follows:\n\n$X_{Y;\\rho=0} = (0) \\cdot SD (Y^{\\perp})Y + \\sqrt{1- (0)^2}SD(Y)Y^{\\perp}$\n\n$X_{Y;\\rho=0} = \\sqrt{1}SD(Y)Y^{\\perp}$\n\n$X_{Y;\\rho = 0} = SD(Y)Y^{\\perp}$\n",
    "supporting": [
      "002-blog_variance-sum-law_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}